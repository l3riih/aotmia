# üî¨ Sistema de Atomizaci√≥n Independiente - Atomia

## üìã Resumen

El **Sistema de Atomizaci√≥n Independiente** es una implementaci√≥n completa de pipeline modular que convierte documentos largos en √°tomos de aprendizaje coherentes, manteniendo la consistencia conceptual y pedag√≥gica entre divisiones.

## üèóÔ∏è Arquitectura del Pipeline

### Pipeline de 7 Pasos

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  ATOMIZATION PIPELINE                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [Parse] ‚Üí [Chunk] ‚Üí [Atomize] ‚Üí [Relate] ‚Üí [Validate] ‚Üí   ‚îÇ
‚îÇ  [Store] ‚Üí [Index]                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Descripci√≥n de Pasos

1. **Parse** - Extrae contenido de m√∫ltiples formatos (PDF, DOCX, TXT, HTML, MD)
2. **Chunk** - Divisi√≥n jer√°rquica respetando l√≠mites de tokens y coherencia sem√°ntica
3. **Atomize** - Procesamiento con agente educativo para crear √°tomos de aprendizaje
4. **Relate** - Resoluci√≥n de dependencias entre √°tomos de diferentes chunks
5. **Validate** - Validaci√≥n pedag√≥gica y control de calidad
6. **Store** - Persistencia en MongoDB y Neo4j con metadatos
7. **Index** - Creaci√≥n de √≠ndices de b√∫squeda y cache

## üîß Componentes Implementados

### 1. Parsers Multi-Formato (`parsers.py`)

```python
# Formatos soportados
SUPPORTED_PARSERS = {
    "text/plain": TxtParser(),
    "application/pdf": PdfParser(),
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": DocxParser(),
    "text/html": HtmlParser(),
    "text/markdown": MarkdownParser(),
}
```

**Caracter√≠sticas:**
- Auto-detecci√≥n de tipo de contenido
- Manejo robusto de errores
- Extracci√≥n de metadatos
- Dependencias opcionales (graceful degradation)

### 2. Chunker Jer√°rquico (`chunker.py`)

```python
def chunk_text_hierarchical(text: str, max_tokens: int = 4000) -> List[Chunk]:
    """Divisi√≥n inteligente respetando estructura del documento"""
```

**Estrategias:**
- Detecci√≥n de encabezados (Markdown, numerados)
- Divisi√≥n por p√°rrafos como fallback
- Respeto de l√≠mites de tokens
- Preservaci√≥n de jerarqu√≠a conceptual

### 3. Pipeline Orquestador (`pipeline.py`)

```python
async def run_atomization_pipeline(
    raw_data: bytes | str,
    filename: str | None = None,
    content_type: str | None = None,
    objectives: str | None = None,
    difficulty: str = "intermedio",
    user_id: str | None = None
) -> Dict[str, Any]:
```

**Flujo Completo:**
- Contexto compartido entre pasos
- Manejo de errores por paso
- Logging estructurado
- M√©tricas de calidad

### 4. Pasos Avanzados (`steps.py`)

#### RelateStep
- Extracci√≥n de conceptos globales
- Resoluci√≥n de dependencias cruzadas
- Validaci√≥n de grafos (detecci√≥n de ciclos)

#### ValidateStep
- Validaci√≥n de estructura de √°tomos
- Control de calidad pedag√≥gica
- C√°lculo de scores de calidad

#### StoreStep
- Persistencia en MongoDB
- Relaciones en Neo4j
- Metadatos de pipeline

#### IndexStep
- √çndices de b√∫squeda
- Cache de resultados
- Optimizaci√≥n de consultas

## üåê Interfaz Web

### Dashboard de Gesti√≥n (`dashboard.py`)

**Rutas Disponibles:**
- `/dashboard/` - Dashboard principal
- `/dashboard/upload` - Interfaz de subida de archivos
- `/dashboard/monitor/{processing_id}` - Monitoreo en tiempo real
- `/dashboard/results/{result_id}` - Visualizaci√≥n de resultados
- `/dashboard/graph/{result_id}` - Grafo de dependencias

**Caracter√≠sticas:**
- Drag & drop para archivos
- Progreso en tiempo real
- Visualizaci√≥n de m√©tricas
- Grafo interactivo de dependencias

## üöÄ Uso del Sistema

### 1. API Endpoint

```bash
curl -X POST "http://localhost:8001/api/v1/pipeline/process" \
  -F "file=@documento.pdf" \
  -F "objectives=Introducir conceptos de c√°lculo" \
  -F "difficulty=intermedio" \
  -F "user_id=usuario_123"
```

### 2. Interfaz Web

1. Visitar `http://localhost:8001/dashboard/`
2. Navegar a "Subir Documento"
3. Arrastrar archivo o seleccionar
4. Configurar par√°metros
5. Monitorear progreso
6. Revisar resultados

### 3. Program√°ticamente

```python
from atomization.src.domain.pipeline import run_atomization_pipeline

result = await run_atomization_pipeline(
    raw_data=pdf_bytes,
    filename="documento.pdf",
    content_type="application/pdf",
    objectives="Introducir funciones matem√°ticas",
    difficulty="intermedio",
    user_id="user_123"
)

atoms = result["atoms"]
metadata = result["metadata"]
```

## üìä M√©tricas y Monitoreo

### M√©tricas de Calidad

```python
{
    "quality_metrics": {
        "average_quality_score": 0.87,
        "validation_pass_rate": 0.93,
        "dependency_coherence": 0.91
    },
    "processing_metrics": {
        "total_time_seconds": 154,
        "chunks_processed": 4,
        "concepts_identified": 12,
        "cross_chunk_dependencies": 6
    }
}
```

### Logging Estructurado

```json
{
    "event": "pipeline_complete",
    "filename": "documento.pdf",
    "user_id": "user_123",
    "atoms_created": 15,
    "chunks_processed": 4,
    "processing_time": "2m 34s",
    "quality_score": 0.87
}
```

## üîç Resoluci√≥n de Dependencias

### Algoritmo Multi-Pasada

1. **Pasada 1**: Atomizaci√≥n local por chunk
2. **Pasada 2**: Mapeo conceptual global
3. **Pasada 3**: Resoluci√≥n de referencias cruzadas
4. **Pasada 4**: Validaci√≥n de coherencia

### Manejo de Conceptos

```python
{
    "concepto_funcion": {
        "name": "Funci√≥n Matem√°tica",
        "introduced_in": ["atom_001"],
        "referenced_in": ["atom_002", "atom_003"],
        "dependencies": []
    }
}
```

## üõ†Ô∏è Configuraci√≥n

### Variables de Entorno

```bash
# LLM Orchestrator
LLM_ORCHESTRATOR_URL=http://localhost:8002

# Bases de datos
MONGODB_URL=mongodb://localhost:27017
NEO4J_URI=bolt://localhost:7687
REDIS_URL=redis://localhost:6379

# Pipeline
MAX_TOKENS_PER_CHUNK=4000
ENABLE_VALIDATION=true
CACHE_TTL_SECONDS=3600
```

### Dependencias

```bash
pip install python-docx beautifulsoup4 markdown
```

## üß™ Testing

### Tests de Pipeline

```python
# Test completo del pipeline
pytest tests/test_pipeline_integration.py

# Test de parsers espec√≠ficos
pytest tests/test_parsers.py

# Test de chunker
pytest tests/test_chunker.py
```

### Casos de Prueba

1. **Documentos PDF largos** (>50 p√°ginas)
2. **Archivos DOCX con estructura compleja**
3. **Contenido HTML con m√∫ltiples secciones**
4. **Markdown con jerarqu√≠a anidada**
5. **Texto plano sin estructura**

## üö¶ Estado Actual

### ‚úÖ Implementado

- [x] Pipeline completo de 7 pasos
- [x] Parsers para 5 formatos
- [x] Chunker jer√°rquico
- [x] Resoluci√≥n de dependencias
- [x] Validaci√≥n pedag√≥gica
- [x] Interfaz web b√°sica
- [x] API endpoints
- [x] Logging estructurado

### üîÑ En Desarrollo

- [ ] Tests de integraci√≥n completos
- [ ] Optimizaci√≥n de performance
- [ ] Cache avanzado
- [ ] M√©tricas detalladas
- [ ] Documentaci√≥n de API

### üéØ Pr√≥ximas Mejoras

- [ ] Soporte para EPUB
- [ ] Extracci√≥n de im√°genes/diagramas
- [ ] IA para detecci√≥n de estructura
- [ ] Paralelizaci√≥n de chunks
- [ ] Dashboard en tiempo real

## üìà Performance

### Benchmarks Estimados

| Tama√±o Documento | Tiempo Procesamiento | √Åtomos Generados |
|------------------|---------------------|------------------|
| 1-5 p√°ginas      | 30-60 segundos      | 5-15 √°tomos      |
| 5-20 p√°ginas     | 1-3 minutos         | 15-50 √°tomos     |
| 20-50 p√°ginas    | 3-8 minutos         | 50-150 √°tomos    |
| 50+ p√°ginas      | 8-20 minutos        | 150+ √°tomos      |

### Optimizaciones

- **Procesamiento paralelo** de chunks independientes
- **Cache inteligente** de resultados similares
- **Streaming** para documentos muy largos
- **Compresi√≥n** de metadatos

## ü§ù Contribuci√≥n

### Agregar Nuevo Parser

```python
class EpubParser(BaseParser):
    content_type = "application/epub+zip"
    
    def parse(self, data: bytes, *, filename: str | None = None):
        # Implementar extracci√≥n EPUB
        pass

# Registrar en SUPPORTED_PARSERS
SUPPORTED_PARSERS["application/epub+zip"] = EpubParser()
```

### Agregar Paso de Pipeline

```python
class CustomStep(PipelineStep):
    name = "custom"
    
    async def _run(self, context: Dict[str, Any]) -> Dict[str, Any]:
        # Implementar l√≥gica personalizada
        return context

# Agregar al pipeline
steps.append(CustomStep())
```

---

**El Sistema de Atomizaci√≥n Independiente representa un avance significativo en el procesamiento autom√°tico de contenido educativo, combinando t√©cnicas de NLP avanzadas con principios pedag√≥gicos s√≥lidos para crear experiencias de aprendizaje personalizadas y efectivas.** 